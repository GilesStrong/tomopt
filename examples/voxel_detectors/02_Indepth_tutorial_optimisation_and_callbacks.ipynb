{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-depth tutorial: optimisation & callbacks\n",
    "In the single-cycle tutorial we looked at the propagation and inference steps that make up a forward pass, and how the loss can be backpropagated  and the detectors optimised. There's a lot of boilerplate code there, and in the tutorial we'll look at how the we can use a wrapper class to more simply optimise the detectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VolumeWrapper\n",
    "The `Volume` class already simplifies propagating muons through multiple layers and provides a few useful methods, however it doesn't provide all the functionality we would like for working with different detector states. The `tomopt.optimisation.wrapper.volume_wrapper.VoxerVolumeWrapper` takes a `Volume` during initialisation and extends it extra methods to handle optimisation, loading, saving, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tomopt.volume import PassiveLayer, VoxelDetectorLayer, Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eff_cost(x:Tensor) -> Tensor:\n",
    "    return torch.expm1(3*F.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_cost(x:Tensor) -> Tensor:\n",
    "    return F.relu(x/100)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers():\n",
    "    layers = []\n",
    "    lwh = Tensor([1,1,1])\n",
    "    size = 0.1\n",
    "    init_eff = 0.5\n",
    "    init_res = 1000\n",
    "    pos = 'above'\n",
    "    for z,d in zip(np.arange(lwh[2],0,-size), [1,1,0,0,0,0,0,0,1,1]):\n",
    "        if d:\n",
    "            layers.append(VoxelDetectorLayer(pos=pos, init_eff=init_eff, init_res=init_res,\n",
    "                                             lw=lwh[:2], z=z, size=size, eff_cost_func=eff_cost, res_cost_func=res_cost, device=DEVICE))\n",
    "        else:\n",
    "            pos = 'below'\n",
    "            layers.append(PassiveLayer(lw=lwh[:2], z=z, size=size, device=DEVICE))\n",
    "\n",
    "    return nn.ModuleList(layers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = Volume(get_layers())\n",
    "volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tomopt.optimisation import VoxelVolumeWrapper, DetectorLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, when building the wrapper, we supply optimisers for both the resolution and efficiency, and also the loss function we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = VoxelVolumeWrapper(volume,\n",
    "                             res_opt=partial(torch.optim.SGD, lr=2e9),  # Scale of resolution is large, so use high LR\n",
    "                             eff_opt=partial(torch.optim.SGD, lr=2e4),  # Scale of efficiency is [0,1] so use small LR\n",
    "                             loss_func=DetectorLoss(target_budget=0.8, cost_coef=None))  # Loss is precision + budget_coef*cost_coef*detector cost, balance coef as required or leave as None to automatically balance on first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.get_detectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.get_param_count(trainable=True)  # number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.get_param_count(trainable=False)  # total number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter states of the detectors and their optimisers can be saved and loaded via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.save('save.pt')\n",
    "wrapper.load('save.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a class-method for instantiating directly from a save (volume and optimisers still need to be passed, since only the parameters states are saved):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VoxelVolumeWrapper.from_save('save.pt', volume=volume, res_opt=partial(torch.optim.SGD, lr=2e10), eff_opt=partial(torch.optim.SGD, lr=2e5),\n",
    "                             loss_func=DetectorLoss(cost_coef=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate and momentum/$\\beta_1$ of the optimisers are accessible as parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wrapper.get_opt_lr('eff_opt'), wrapper.get_opt_mom('eff_opt'), wrapper.get_opt_lr('res_opt'), wrapper.get_opt_mom('res_opt'))\n",
    "wrapper.set_opt_lr(1e-4, 'eff_opt')\n",
    "print(wrapper.get_opt_lr('eff_opt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.load('save.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation\n",
    "As discussed in the Hello World tutorial, optimisation of the detector configuration runs over several loops: multiple batches of muons can be used to construct inference of the x0 of a given passive volume, and the loss can be accumulated over multiple different passive volumes. Additionally, there can different sets of passive volumes for training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passive volumes\n",
    "Passive volumes are held in `tomopt.optimisation.data.passives.PassiveYielder` objects. These iterate over a (shuffled) list of fixed passive volumes. In the future they will also be able to generate and yield a set number of randomised volumes using the `tomopt.optimisation.data.passives.PassiveGenerator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.core import X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arb_rad_length(*,z:float, lw:Tensor, size:float) -> float:\n",
    "    rad_length = torch.ones(list((lw/size).long()))*X0['beryllium']\n",
    "    if z >= 0.4 and z <= 0.5: rad_length[5:,5:] = X0['lead']\n",
    "    return rad_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arb_rad_length2(*,z:float, lw:Tensor, size:float) -> float:\n",
    "    rad_length = torch.ones(list((lw/size).long()))*X0['beryllium']\n",
    "    if z >= 0.4 and z <= 0.5: rad_length[5:,5:] = X0['copper']\n",
    "    return rad_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.optimisation import PassiveYielder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passives = PassiveYielder([arb_rad_length,arb_rad_length2], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in passives: print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting loop\n",
    "The fitting loop initiated via `VolumeWrapper.fit` is:\n",
    "\n",
    "1. for epoch in `n_epochs`:\n",
    "    1. `loss` = 0\n",
    "    1. for `p`, `passive` in enumerate(`trn_passives`):\n",
    "        1. load `passive` into passive volume\n",
    "        1. for muon_batch in range(`n_mu_per_volume`//`mu_bs`):\n",
    "            1. Irradiate volume with `mu_bs` muons\n",
    "            1. Infer passive volume\n",
    "        1. Compute loss based on precision and cost, and add to `loss`\n",
    "        1. if `p`+1 % `passive_bs` == 0:\n",
    "            1. `loss` = `loss`/`passive_bs`\n",
    "            1. Backpropagate `loss` and update detector parameters\n",
    "            1. `loss` = 0\n",
    "        1. if len(`trn_passives`)-(`p`+1) < `passive_bs`:\n",
    "            1. Break\n",
    "            \n",
    "    1. `val_loss` = 0\n",
    "    1. for `p`, `passive` in enumerate(`val_passives`):\n",
    "        1. load `passive` into passive volume\n",
    "        1. for muon_batch in range(`n_mu_per_volume`//`mu_bs`):\n",
    "            1. Irradiate volume with `mu_bs` muons\n",
    "            1. Infer passive volume\n",
    "            1. Compute loss based on precision and cost, and add to `val_loss`\n",
    "        1. if len(`val_passives`)-(`p`+1) < `passive_bs`:\n",
    "            1. Break\n",
    "    1. `val_loss` = `val_loss`/`p`\n",
    "\n",
    "I.e. the loss for a single passive can be computed over several iterations of muon irradiation (for computational speed), and the total loss can be accumulated over multiple passives before updating the detector. We also have the option of evaluating the detector on unseen passives via validation data. For this example, we'll use the same passive for the both training and validation.\n",
    "\n",
    "In implementation, the loop is broken up into several functions:\n",
    "- `_fit_epoch` runs one full epoch of volumes and updates for both training and validation\n",
    "- `_scan_volumes` runs over all training/validation volumes, updating parameters when necessary\n",
    "- `_scan_volume` irradiates a single volume with muons multiple batches, and computes the loss for that volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Callbacks and stateful optimisation\n",
    "The optimisation loop has many interjection points for callback classes. These are class which can be used to affect most aspects of the optimisation, without having to modify the code itself. Additionally, the optimisation loop is *stateful*, meaning that all aspects of the optimisation (data, losses, infrence classes, etc.) are attributes of a state class `VolumeWrapper.fit_params`. Every callback will have the `VolumeWrapper` as an attribute, and so every callback can access the `fit_params` object and so can read and modify these states. Additionally, since the callbacks are themselves an attribute of `fit_params`, every callback has access to every other callback. This carries several advantages:\n",
    "- Callbacks are able to adjust their logic based on the presence and order of other callbacks.\n",
    "- Callbacks can adjust their logic according to whether the optimisation loop in in \"train\", \"valid\" or \"test\" state (`fit_params.state`).\n",
    "- Information accumulated by one callback can be used by another, without explicitly sharing it.\n",
    "- Interjection calls are cleaner since no arguments need to be passed, and users aren't restricted by what the devs thought were necessary to be passed.\n",
    "- Optimiser functions are cleaner, since callbacks read the majority of, what would otherwise be arguments, from the `fit_params`.\n",
    "\n",
    "Accounting for the interjection calls (`on_*_begin` & `on_*_end`), the full optimisation loop is:\n",
    "1. Associate callbacks with wrapper (`set_wrapper`)\n",
    "1. `on_train_begin`\n",
    "1. for epoch in `n_epochs`:\n",
    "    1. `state` = \"train\"\n",
    "    1. `on_epoch_begin`\n",
    "    1. for `p`, `passive` in enumerate(`trn_passives`):\n",
    "        1. if `p` % `passive_bs` == 0:\n",
    "            1. `on_volume_batch_begin`\n",
    "            1. `loss` = 0\n",
    "        1. load `passive` into passive volume\n",
    "        1. `on_volume_begin`\n",
    "        1. for muon_batch in range(`n_mu_per_volume`//`mu_bs`):\n",
    "            1. `on_mu_batch_begin`\n",
    "            1. Irradiate volume with `mu_bs` muons\n",
    "            1. Infer scatter locations\n",
    "            1. `on_scatter_end`\n",
    "            1. Infer x0 and append to list of x0 predictions\n",
    "            1. `on_mu_batch_end`\n",
    "        1. `on_x0_pred_begin`\n",
    "        1. Compute overall x0 prediction\n",
    "        1. `on_x0_pred_end`\n",
    "        1. Compute loss based on precision and cost, and add to `loss`\n",
    "        1. if `p`+1 % `passive_bs` == 0:\n",
    "            1. `loss` = `loss`/`passive_bs`\n",
    "            1. `on_volume_batch_end`\n",
    "            1. Zero parameter gradients\n",
    "            1. `on_backwards_begin`\n",
    "            1. Backpropagate `loss` and compute parameter gradients\n",
    "            1. `on_backwards_end`\n",
    "            1. Update detector parameters\n",
    "            1. Ensure detector parameters are within physical boundaries (`AbsDetectorLayer.conform_detector`)\n",
    "            1. `loss` = 0\n",
    "        1. if len(`trn_passives`)-(`p`+1) < `passive_bs`:\n",
    "            1. Break\n",
    "    1. `on_epoch_end`\n",
    "    1. `state` = \"valid\"\n",
    "    1. `on_epoch_begin`\n",
    "    1. for `p`, `passive` in enumerate(`val_passives`):\n",
    "        1. if `p` % `passive_bs` == 0:\n",
    "            1. `on_volume_batch_begin`\n",
    "            1. `loss` = 0\n",
    "        1. `on_volume_begin`\n",
    "        1. for muon_batch in range(`n_mu_per_volume`//`mu_bs`):\n",
    "            1. `on_mu_batch_begin`\n",
    "            1. Irradiate volume with `mu_bs` muons\n",
    "            1. Infer scatter locations\n",
    "            1. `on_scatter_end`\n",
    "            1. Infer x0 and append to list of x0 predictions\n",
    "            1. `on_mu_batch_end`\n",
    "        1. `on_x0_pred_begin`\n",
    "        1. Compute overall x0 prediction\n",
    "        1. `on_x0_pred_end`\n",
    "        1. Compute loss based on precision and cost, and add to `loss`\n",
    "        1. if `p`+1 % `passive_bs` == 0:\n",
    "            1. `loss` = `loss`/`passive_bs`\n",
    "            1. `on_volume_batch_end`\n",
    "        1. if len(`val_passives`)-(`p`+1) < `passive_bs`:\n",
    "            1. Break\n",
    "    1. `on_epoch_end`\n",
    "1. `on_train_end`\n",
    "\n",
    "Callbacks are aimed to be user-writable to modify the existing code to their specific interests and requirements. Some callbacks will be included in TomOpt, e.g. `tomopt.optimisation.callbacks.grad_callbacks.NoMoreNaNs`:\n",
    "\n",
    "```python\n",
    "class NoMoreNaNs(Callback):\n",
    "    def on_backwards_end(self) -> None:\n",
    "        for l in self.wrapper.volume.get_detectors():\n",
    "            torch.nan_to_num_(l.resolution.grad, 0)\n",
    "            torch.nan_to_num_(l.efficiency.grad, 0)\n",
    "```\n",
    "\n",
    "This only acts at one interjection point (the others are implemented in the parent `tomopt.optimisation.callbacks.callback.Callback` class as `pass`), and modifies the gradients of the volume parameters to remove `NaN` values, which sometimes occur.\n",
    "\n",
    "Several specific subclasses of `Callback` are specifically looked for, and filtered into lists of callbacks in `VolumeWrapper.fit_params`:\n",
    "- `tomopt.optimisation.callbacks.cyclic_callbacks.CyclicCallback` - callbacks that modify optimiser parameters in a cyclical manner, e.g. cosine annealing and 1cycle, and may be used to sync other callbacks to their cycle, e.g. to delay early stopping until a cycle has ended.\n",
    "- `tomopt.optimisation.callbacks.eval_metric.EvalMetric` - callbacks designed to compute performance metrics\n",
    "- `tomopt.optimisation.callbacks.monitors.MetricLogger` - a special callback that tracks training and validation losses and the results of the `EvalMetric`s, and displays realtime information during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting\n",
    "Let's run the optimisation. As mentioned before, sometimes `NaN` gradients can occur, so we'll use `NoMoreNaNs` to set them to zero, and we'll also use a `MetricLogger` to get realtime information and a history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.optimisation import NoMoreNaNs, VoxelMetricLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = VoxelMetricLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passives = PassiveYielder([arb_rad_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = wrapper.fit(n_epochs=10,  # Run for 10 epochs\n",
    "                passive_bs=1,  # Update every passive volumes (we only have one)\n",
    "                n_mu_per_volume=1000,  # Use 1000 per passive volume\n",
    "                mu_bs=100,  # Run the 1000 muons per volume in 10 batches of 100 muons (quicker than a single batch of 1000)\n",
    "                trn_passives=passives,\n",
    "                val_passives=passives,  # Evaluate on the training data\n",
    "                cbs=[NoMoreNaNs(),ml])  # Use these two callbacks during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.get_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.get_results(loaded_best=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Having optimised the detector, we now want to use it to predict the composition of passive volumes without updating; since we're still in simulation, we can assume that we'll always have access to the true composition.\n",
    "`VolumeWrapper.predict` can be used to run the predictions for each volume in a `PassiveYielder`, and returns a list of pairs of numpy arrays; `[0]` is the prediction, and `[1]` is the truth.\n",
    "The function reuses a lot of the code used during fitting, running in `fit_params.state = \"test\"` mode. This means that we still have the functionality to run muons in smaller batches, and the use of callbacks. Indeed, the predictions themselves are collected by a special callback `tomopt.optimisation.callbacks.pred_callbacks.PredHandler`:\n",
    "\n",
    "```python\n",
    "class PredHandler(Callback):\n",
    "    r\"\"\"\n",
    "    Default callback for predictions. Collects predictions for a range of volumes and returns them as list of numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    def on_pred_begin(self) -> None:\n",
    "        super().__init__()\n",
    "        self.preds: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "\n",
    "    def get_preds(self) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        return self.preds\n",
    "\n",
    "    def on_x0_pred_end(self) -> None:\n",
    "        if self.wrapper.fit_params.state == \"test\":\n",
    "            self.preds.append((self.wrapper.fit_params.pred.detach().cpu().numpy(), self.wrapper.volume.get_rad_cube().detach().cpu().numpy()))\n",
    "```\n",
    "\n",
    "Whilst this is the default prediction callback, we are free to pass our own prediction callbacks to the prediction function as a separate argument to any other callback we wish to use. When running predictions, `on_train_*` methods are not called, instead `on_pred_begin` and `on_pred_end` are called at similar points.\n",
    "\n",
    "As well as just running inference of the passive volume, we may wish to gather diagnostic information about the hits and scatter locations. `tomopt.optimisation.callbacks.diagnostic_callbacks` contains:\n",
    "- `ScatterRecord` to record the reconstructed scatter locations in the passive volume\n",
    "- `HitRecord` to record the reconstructed hit locations in the detectors\n",
    "\n",
    "Warning: currently these are really only valid when running predictions for a single passive volume (like we will)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.optimisation import PredHandler, ScatterRecord, HitRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = ScatterRecord()\n",
    "hr = HitRecord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = wrapper.predict(passives, n_mu_per_volume=10000, mu_bs=100, pred_cb=PredHandler(), cbs=[sr,hr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X0 prediction\n",
    "We can plot the predictions per passive volume (although we only have one) using `tomopt.plotting.predictions.plot_pred_true_x0`, which takes a single prediction and truth pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.plotting import plot_pred_true_x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_true_x0(preds[0][0], preds[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the outline of the lead block, however due to the low number of muons used, and the high uncertainty in z, the difference extends over several layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnostics\n",
    "Since we passed the `*Record` callbacks during prediction, they will have accumulated their relevant information which we can access by calling their `get_record` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_df = sr.get_record(as_df=True)\n",
    "scatter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_df = hr.get_record(as_df=True)\n",
    "hit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot these to get the densities of hits and scatters per layer using the plotting functions in `tomopt.plotting.diagnostics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.plotting import plot_hit_density, plot_scatter_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_density(scatter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hit_density(hit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From both of these plots we can see then density along the edges is lower, this is due to the fact that hits and scatters are recorded only for muons which stay within the passive volume, so muons which do pass through the edges in one layer are likely to leave the volume and be excluded from the plots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tomopt]",
   "language": "python",
   "name": "conda-env-tomopt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
