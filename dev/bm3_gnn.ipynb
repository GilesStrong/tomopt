{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM3 GNN Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tomopt.volume import *\n",
    "from tomopt.muon import *\n",
    "from tomopt.inference import *\n",
    "from tomopt.optimisation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.volume.layer import Layer\n",
    "\n",
    "def get_volume(size: float = 0.2, lwh: Tensor = Tensor([1.0, 1.0, 1.4]), device: torch.device = torch.device(\"cpu\")) -> Volume:\n",
    "    def area_cost(x: Tensor) -> Tensor:\n",
    "        return F.relu(x)\n",
    "\n",
    "    layers: List[Layer] = []\n",
    "    n_panels = 4\n",
    "    layers.append(\n",
    "        PanelDetectorLayer(\n",
    "            pos=\"above\",\n",
    "            lw=lwh[:2],\n",
    "            z=lwh[2].item(),\n",
    "            size=size,\n",
    "            panels=[\n",
    "                DetectorPanel(\n",
    "                    res=1e3, eff=1, init_xyz=(0.5, 0.5, 1 - (i * (size) / n_panels)), init_xy_span=(1.0, 1.0), area_cost_func=area_cost, device=device\n",
    "                )\n",
    "                for i in range(n_panels)\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "    for z in np.round(np.arange(lwh[2] - size, size, -size), decimals=1):\n",
    "        layers.append(PassiveLayer(lw=lwh[:2], z=z, size=size, device=device))\n",
    "    layers.append(\n",
    "        PanelDetectorLayer(\n",
    "            pos=\"below\",\n",
    "            lw=lwh[:2],\n",
    "            z=size,\n",
    "            size=size,\n",
    "            panels=[\n",
    "                DetectorPanel(\n",
    "                    res=1e3, eff=1, init_xyz=(0.5, 0.5, 0.2 - (i * (size) / n_panels)), init_xy_span=(1.0, 1.0), area_cost_func=area_cost, device=device\n",
    "                )\n",
    "                for i in range(n_panels)\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return Volume(nn.ModuleList(layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = get_volume(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "muons = MuonBatch(generate_batch(250), init_z=volume.h, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume(muons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = PanelScatterBatch(muons, volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN inferer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load('../../mode_muon_tomo_inference/dev/exported_models/bm3_traced.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.jit._script.RecursiveScriptModule"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.inference.volume import AbsVolumeInferer, AbsX0Inferer\n",
    "from tomopt.inference.scattering import AbsScatterBatch\n",
    "\n",
    "\n",
    "class DeepVolumeInferer(AbsVolumeInferer):\n",
    "    def __init__(self, model: Union[torch.jit._script.RecursiveScriptModule, nn.Module], base_inferer:AbsX0Inferer, volume: Volume):\n",
    "        super().__init__(volume=volume)\n",
    "        self.model,self.base_inferer = model,base_inferer\n",
    "        self.voxel_centres = self._build_centres()\n",
    "        \n",
    "        self.in_vars: List[Tensor] = []\n",
    "        self.in_var_uncs: List[Tensor] = []\n",
    "        self.efficiencies: List[Tensor] = []\n",
    "        self.in_var: Optional[Tensor] = None\n",
    "        self.in_var_unc: Optional[Tensor] = None\n",
    "        self.efficiency: Optional[Tensor] = None\n",
    "        \n",
    "    def compute_efficiency(self, scatters:AbsScatterBatch) -> Tensor:\n",
    "        return self.base_inferer.compute_efficiency(scatters=scatters)\n",
    "    \n",
    "    def get_base_predictions(self, scatters:AbsScatterBatch) -> Tensor:\n",
    "        x, u = self.base_inferer.x0_from_dtheta(scatters=scatters)\n",
    "        return x[:, None], u[:, None]\n",
    "    \n",
    "    def add_scatters(self, scatters: AbsScatterBatch) -> None:\n",
    "        super().add_scatters(scatters=scatters)\n",
    "        x0, x0_unc = self.get_base_predictions(scatters)\n",
    "        self.in_vars.append(torch.cat((sb.dtheta, sb.dxy, x0, sb.location), dim=-1))\n",
    "        self.in_var_uncs.append(torch.cat((sb.dtheta_unc, sb.dxy_unc, x0_unc, sb.location_unc), dim=-1))\n",
    "        self.efficiencies.append(self.compute_efficiency(scatters=scatters))\n",
    "        \n",
    "    def _build_centres(self) -> Tensor:\n",
    "        bounds = (\n",
    "            self.volume.passive_size\n",
    "            * np.mgrid[\n",
    "                round(self.volume.get_passive_z_range()[0].detach().cpu().numpy()[0] / self.volume.passive_size) : round(\n",
    "                    self.volume.get_passive_z_range()[1].detach().cpu().numpy()[0] / self.volume.passive_size\n",
    "                ) : 1,\n",
    "                0 : round(self.volume.lw.detach().cpu().numpy()[0] / self.volume.passive_size) : 1,\n",
    "                0 : round(self.volume.lw.detach().cpu().numpy()[1] / self.volume.passive_size) : 1,\n",
    "            ]\n",
    "        )\n",
    "        #         bounds[0] = np.flip(bounds[0])  # z is reversed\n",
    "        return torch.tensor(bounds.reshape(3, -1).transpose(-1, -2), dtype=torch.float32) + (self.volume.passive_size / 2)\n",
    "        \n",
    "    def _build_inputs(self, in_var:Tensor) -> Tensor:\n",
    "        data = in_var[None, :].repeat_interleave(len(self.voxel_centres), dim=0)\n",
    "        data[:, :, -3:] -= self.voxel_centres[:, None].repeat_interleave(len(in_var), dim=1)\n",
    "        data = torch.cat((data, torch.norm(data[:, :, -3:], dim=-1, keepdim=True)), dim=-1)  # dR\n",
    "        return data\n",
    "    \n",
    "    def _get_weight(self) -> Tensor:\n",
    "        '''Maybe alter this to include resolution/pred uncertainties'''\n",
    "        return self.efficiency\n",
    "        \n",
    "    def get_prediction(self) -> Tuple[Optional[Tensor], Optional[Tensor]]:\n",
    "        self.in_var = torch.cat(self.in_vars, dim=0)\n",
    "        self.in_var_unc = torch.cat(self.in_var_uncs, dim=0)\n",
    "        self.efficiency = torch.cat(self.efficiencies, dim=0)\n",
    "        \n",
    "        inputs = self._build_inputs(self.in_var)\n",
    "        pred = self.model(inputs[None])\n",
    "        weight = self._get_weight()\n",
    "        return pred, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvi = DeepVolumeInferer(model=model, base_inferer=PanelX0Inferer(volume), volume=volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvi.in_vars, dvi.in_var_uncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvi.add_scatters(sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([250, 8]), torch.Size([250, 8]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvi.in_vars[0].shape, dvi.in_var_uncs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.4 s, sys: 623 ms, total: 54 s\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p,w = dvi.get_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 125])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(p).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.7199, -4.6612, -4.8523, -5.3027, -5.9038, -4.4587, -4.3215,\n",
       "          -4.4367, -4.8904, -5.5355, -4.4805, -4.3712, -4.4131, -4.7845,\n",
       "          -5.3617, -4.7846, -4.6841, -4.7248, -5.0203, -5.5507, -5.3168,\n",
       "          -5.2353, -5.3210, -5.5242, -5.9422, -3.1803, -3.0079, -3.1728,\n",
       "          -3.7733, -4.6853, -2.7979, -2.5219, -2.6782, -3.2331, -4.2915,\n",
       "          -2.7714, -2.4809, -2.5953, -3.0754, -4.0568, -3.0980, -2.7760,\n",
       "          -2.8489, -3.3317, -4.1621, -3.7012, -3.4231, -3.5473, -4.0163,\n",
       "          -4.6220, -2.0405, -1.6804, -1.8288, -2.4105, -3.3542, -1.5385,\n",
       "          -1.4254, -1.5604, -1.9171, -2.8633, -1.4701, -1.3884, -1.5540,\n",
       "          -1.8216, -2.5975, -1.6655, -1.5257, -1.6327, -2.0116, -2.8083,\n",
       "          -2.3029, -2.1007, -2.1466, -2.6024, -3.3770, -1.5095, -1.1738,\n",
       "          -1.2140, -1.6123, -2.3049, -1.1016, -0.8568, -0.8933, -1.1963,\n",
       "          -2.0857, -0.9639, -0.8060, -0.9017, -1.1481, -1.9760, -1.0776,\n",
       "          -0.9097, -1.0306, -1.3157, -2.0047, -1.6967, -1.3401, -1.4325,\n",
       "          -1.9252, -2.5842, -1.4498, -1.1253, -1.0925, -1.2832, -1.6213,\n",
       "          -1.1299, -0.8555, -0.8298, -1.0156, -1.4335, -1.0557, -0.7485,\n",
       "          -0.7506, -0.9994, -1.4171, -1.2714, -0.8688, -0.8843, -1.1706,\n",
       "          -1.5762, -1.9514, -1.4183, -1.4649, -1.8299, -2.2685],\n",
       "         [-4.7301, -4.3587, -4.3450, -4.7172, -5.4811, -4.2176, -3.7469,\n",
       "          -3.6341, -4.0270, -4.8671, -4.1173, -3.6276, -3.4789, -3.8178,\n",
       "          -4.5943, -4.3594, -3.9303, -3.7915, -4.0461, -4.9323, -5.0017,\n",
       "          -4.5924, -4.4965, -4.8367, -5.7232, -3.4297, -2.9545, -2.8976,\n",
       "          -3.3555, -4.3378, -2.7921, -2.2135, -2.1108, -2.5547, -3.6729,\n",
       "          -2.6665, -2.0501, -1.9330, -2.2982, -3.3253, -2.9811, -2.3500,\n",
       "          -2.2076, -2.5792, -3.5289, -3.7228, -3.1200, -3.0057, -3.3959,\n",
       "          -4.4207, -2.6983, -2.0395, -1.9551, -2.4046, -3.3513, -2.0347,\n",
       "          -1.5829, -1.4598, -1.7337, -2.7093, -1.9067, -1.4648, -1.3349,\n",
       "          -1.5614, -2.3912, -2.1415, -1.6316, -1.4717, -1.7544, -2.6810,\n",
       "          -2.9005, -2.3635, -2.1926, -2.5515, -3.6141, -2.3994, -1.9467,\n",
       "          -1.8714, -2.1020, -2.8200, -2.0127, -1.5898, -1.5654, -1.6689,\n",
       "          -2.3781, -1.9747, -1.5955, -1.4663, -1.5464, -2.2690, -2.1557,\n",
       "          -1.6949, -1.5550, -1.7011, -2.4704, -2.7873, -2.2110, -2.0441,\n",
       "          -2.3913, -3.4529, -2.5466, -2.1055, -1.9398, -2.0338, -2.5311,\n",
       "          -2.2140, -1.8060, -1.7291, -1.8260, -2.3717, -2.2326, -1.8470,\n",
       "          -1.7090, -1.8509, -2.4174, -2.5135, -2.0481, -1.8961, -2.1205,\n",
       "          -2.7247, -3.3258, -2.6622, -2.6443, -3.0838, -3.8885],\n",
       "         [-3.6540, -3.5103, -3.5688, -3.8593, -4.3134, -3.4602, -3.1253,\n",
       "          -3.0109, -3.3382, -3.9356, -3.4899, -3.0603, -2.9348, -3.1758,\n",
       "          -3.7147, -3.7094, -3.3420, -3.2384, -3.4050, -3.9797, -4.1189,\n",
       "          -3.8608, -3.7773, -3.9508, -4.4844, -2.7900, -2.5935, -2.6429,\n",
       "          -3.0023, -3.6470, -2.5707, -2.1070, -1.9846, -2.3346, -3.2147,\n",
       "          -2.5399, -1.9803, -1.7973, -2.0493, -2.8869, -2.8214, -2.2850,\n",
       "          -2.0830, -2.2965, -3.0480, -3.3949, -3.0442, -2.8253, -3.0107,\n",
       "          -3.7503, -2.2197, -1.8962, -1.9449, -2.2256, -2.8492, -1.9694,\n",
       "          -1.6907, -1.6016, -1.8653, -2.5620, -1.9790, -1.6390, -1.4911,\n",
       "          -1.6706, -2.3857, -2.2052, -1.8223, -1.6567, -1.8733, -2.6282,\n",
       "          -2.7368, -2.5053, -2.3524, -2.6201, -3.3801, -1.8253, -1.7720,\n",
       "          -1.8278, -1.9407, -2.2863, -1.7408, -1.7412, -1.7596, -1.8638,\n",
       "          -2.2458, -1.8237, -1.7535, -1.7080, -1.8203, -2.2813, -1.9990,\n",
       "          -1.8453, -1.8051, -1.9991, -2.5230, -2.2984, -2.2078, -2.1910,\n",
       "          -2.4744, -3.0651, -1.7571, -1.6963, -1.7153, -1.7796, -1.9405,\n",
       "          -1.7273, -1.6676, -1.7319, -1.8617, -2.0132, -1.7710, -1.7553,\n",
       "          -1.8021, -1.9007, -2.1004, -1.9184, -1.8521, -1.8813, -2.1034,\n",
       "          -2.3478, -2.3270, -2.0932, -2.2171, -2.4859, -2.8761],\n",
       "         [-0.1326, -0.2878, -0.4345, -0.4604, -0.3693, -0.2774, -0.5924,\n",
       "          -0.7989, -0.7933, -0.5955, -0.3628, -0.7716, -1.0076, -0.9807,\n",
       "          -0.6679, -0.3082, -0.6829, -0.9304, -0.8381, -0.5494, -0.1774,\n",
       "          -0.3999, -0.5567, -0.4817, -0.2836, -0.1931, -0.3392, -0.4392,\n",
       "          -0.3928, -0.2581, -0.3514, -0.7270, -0.9375, -0.7919, -0.5077,\n",
       "          -0.4344, -0.9352, -1.1493, -0.9931, -0.6454, -0.3302, -0.7470,\n",
       "          -0.9757, -0.8694, -0.5146, -0.1607, -0.3702, -0.5286, -0.4597,\n",
       "          -0.2399, -0.3794, -0.6744, -0.6674, -0.4119, -0.1861, -0.7021,\n",
       "          -1.1188, -1.2363, -0.8603, -0.3561, -0.7783, -1.2919, -1.4798,\n",
       "          -1.1080, -0.4758, -0.5784, -0.9911, -1.1735, -0.8593, -0.3559,\n",
       "          -0.2665, -0.4086, -0.4756, -0.3513, -0.1490, -0.6445, -0.9803,\n",
       "          -0.9563, -0.6368, -0.3098, -1.0322, -1.6464, -1.5911, -1.0675,\n",
       "          -0.4080, -1.1521, -1.7528, -1.7437, -1.2182, -0.4515, -0.9015,\n",
       "          -1.3804, -1.3505, -0.9151, -0.3751, -0.4271, -0.6645, -0.6681,\n",
       "          -0.4028, -0.1735, -0.6667, -0.9958, -1.0784, -0.8620, -0.5483,\n",
       "          -0.9439, -1.5084, -1.5663, -1.1382, -0.6281, -0.9835, -1.6305,\n",
       "          -1.7079, -1.1263, -0.6074, -0.7112, -1.2241, -1.2602, -0.8063,\n",
       "          -0.4606, -0.3238, -0.5729, -0.5313, -0.3436, -0.2002],\n",
       "         [-2.5189, -1.6197, -1.1926, -1.0968, -1.2430, -1.6902, -1.0045,\n",
       "          -0.7707, -0.7199, -0.8734, -1.4019, -0.7948, -0.6184, -0.5927,\n",
       "          -0.8029, -1.5160, -0.8425, -0.6256, -0.6730, -0.9342, -2.0050,\n",
       "          -1.2272, -0.9469, -1.0467, -1.4708, -3.2133, -2.1949, -1.6754,\n",
       "          -1.5265, -1.7196, -2.3270, -1.5827, -1.2690, -1.0995, -1.1430,\n",
       "          -1.9563, -1.3584, -1.2105, -1.0381, -1.0037, -2.0730, -1.3212,\n",
       "          -1.1068, -0.9980, -1.1715, -2.7179, -1.6874, -1.2985, -1.3178,\n",
       "          -1.7842, -4.6243, -3.7284, -3.1748, -3.0057, -3.1707, -3.9370,\n",
       "          -3.1433, -2.7236, -2.3184, -2.3149, -3.7238, -2.9864, -2.6224,\n",
       "          -2.2069, -2.1153, -3.7987, -2.9168, -2.5900, -2.1530, -2.3180,\n",
       "          -4.2566, -3.2883, -2.9037, -2.6400, -3.1406, -6.1626, -5.8501,\n",
       "          -5.5175, -5.1526, -5.2176, -6.0207, -5.6666, -5.1342, -4.5758,\n",
       "          -4.4030, -6.0352, -5.4919, -4.9255, -4.4225, -3.9566, -6.0688,\n",
       "          -5.4470, -4.7716, -4.3191, -4.3217, -6.0603, -5.5032, -4.9159,\n",
       "          -4.6064, -5.1984, -6.8485, -7.0517, -7.0776, -6.9045, -6.7012,\n",
       "          -7.1388, -7.3462, -7.3081, -6.9512, -6.8691, -7.3290, -7.5073,\n",
       "          -7.2875, -6.7731, -6.6686, -7.2713, -7.4097, -7.0644, -6.6564,\n",
       "          -6.7222, -6.9209, -6.9891, -6.8175, -6.6219, -6.7855]]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABCMeta\n",
    "\n",
    "class AbsDetectorLoss(nn.Module, metaclass=ABCMeta):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        target_budget: float,\n",
    "        budget_smoothing: float = 10,\n",
    "        cost_coef: Optional[Union[Tensor, float]] = None,\n",
    "        steep_budget: bool = True,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.target_budget, self.budget_smoothing, self.cost_coef, self.steep_budget, self.debug = (\n",
    "            target_budget,\n",
    "            budget_smoothing,\n",
    "            cost_coef,\n",
    "            steep_budget,\n",
    "            debug,\n",
    "        )\n",
    "        self.sub_losses: Dict[str, Tensor] = {}  # Store subcomponents in dict for telemetry\n",
    "\n",
    "    def _get_budget_coef(self, cost: Tensor) -> Tensor:\n",
    "        r\"\"\"Switch-on near target budget, plus linear increase above budget\"\"\"\n",
    "\n",
    "        if self.target_budget is None:\n",
    "            return cost.new_zeros(1)\n",
    "\n",
    "        if self.steep_budget:\n",
    "            d = self.budget_smoothing * (cost - self.target_budget) / self.target_budget\n",
    "            if d <= 0:\n",
    "                return 2 * torch.sigmoid(d)\n",
    "            else:\n",
    "                return 1 + (d / 2)\n",
    "        else:\n",
    "            d = cost - self.target_budget\n",
    "            return (2 * torch.sigmoid(self.budget_smoothing * d / self.target_budget)) + (F.relu(d) / self.target_budget)\n",
    "\n",
    "    def _compute_cost_coef(self, inference: Tensor) -> None:\n",
    "        self.cost_coef = inference.detach().clone()\n",
    "        print(f\"Automatically setting cost coefficient to {self.cost_coef}\")\n",
    "        \n",
    "    @abstractmethod\n",
    "    def _get_inference_loss(self, pred:Tensor, pred_weight: Tensor, volume: Volume) -> Tensor:\n",
    "        pass\n",
    "    \n",
    "    def _get_cost_loss(self, volume: Volume) -> Tensor:\n",
    "        if self.cost_coef is None:\n",
    "            self._compute_cost_coef(self.sub_losses[\"error\"])\n",
    "        cost = volume.get_cost()\n",
    "        cost_loss = self._get_budget_coef(cost) * self.cost_coef\n",
    "        if self.debug:\n",
    "            print(\n",
    "                f'cost {cost}, cost coef {self.cost_coef}, budget coef {self._get_budget_coef(cost)}. error loss {self.sub_losses[\"error\"]}, cost loss {self.sub_losses[\"cost\"]}'\n",
    "            )\n",
    "        return cost_loss\n",
    "\n",
    "    def forward(self, pred: Tensor, pred_weight: Tensor, volume: Volume) -> Tensor:\n",
    "        self.sub_losses = {}\n",
    "        self.sub_losses[\"error\"] = self._get_inference_loss(pred, pred_weight, volume)\n",
    "        self.sub_losses[\"cost\"] = self._get_cost_loss(volume)\n",
    "        return self.sub_losses[\"error\"] + self.sub_losses[\"cost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelX0Loss(AbsDetectorLoss):\n",
    "    def _get_inference_loss(self, pred:Tensor, pred_weight: Tensor, volume: Volume) -> Tensor:\n",
    "        true_x0 = volume.get_rad_cube()\n",
    "        return torch.mean(F.mse_loss(pred, true_x0, reduction='none') / pred_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsMaterialClassLoss(AbsDetectorLoss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        x02id: Dict[float, int],\n",
    "        target_budget: float,\n",
    "        budget_smoothing: float = 10,\n",
    "        cost_coef: Optional[Union[Tensor, float]] = None,\n",
    "        steep_budget: bool = True,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        super().__init__(target_budget=target_budget, budget_smoothing=budget_smoothing, cost_coef=cost_coef, steep_budget=steep_budget, debug=debug)\n",
    "        self.x02id = x02id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelClassLoss(AbsMaterialClassLoss):\n",
    "    def _get_inference_loss(self, pred:Tensor, pred_weight: Tensor, volume: Volume) -> Tensor:\n",
    "        true_x0 = volume.get_rad_cube()\n",
    "        for x0 in true_x0.unique():\n",
    "            true_x0[true_x0 == x0] = self.x02id[x0]\n",
    "        true_x0 = true_x0.long()        \n",
    "        return torch.mean(F.nll_loss(pred, true_x0, reduction='none') / pred_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumeClassLoss(AbsMaterialClassLoss):\n",
    "    def _get_inference_loss(self, pred:Tensor, pred_weight: Tensor, volume: Volume) -> Tensor:\n",
    "        targ = volume.target.clone()\n",
    "        for x0 in targ.unique():\n",
    "            targ[targ == x0] = self.x02id[x0]\n",
    "        targ = true_targx0.long()\n",
    "        loss = F.nll_loss(pred, true_x0, reduction='none') if pred.shape[1] > 1 else F.binary_cross_entropy(pred, true_x0, reduction='none')\n",
    "        return torch.mean(loss / pred_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tomopt]",
   "language": "python",
   "name": "conda-env-tomopt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
