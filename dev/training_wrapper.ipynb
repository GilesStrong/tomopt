{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.muon import *\n",
    "from tomopt.inference import *\n",
    "from tomopt.loss import *\n",
    "from tomopt.volume import *\n",
    "from tomopt.core import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import *\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arb_rad_length(*,z:float, lw:Tensor, size:float) -> float:\n",
    "    rad_length = torch.ones(list((lw/size).long()))*X0['beryllium']\n",
    "    if z >= 0.4 and z <= 0.5: rad_length[5:,5:] = X0['lead']\n",
    "    return rad_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eff_cost(x:Tensor) -> Tensor:\n",
    "    return torch.expm1(3*F.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_cost(x:Tensor) -> Tensor:\n",
    "    return F.relu(x/100)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers():\n",
    "    layers = []\n",
    "    lwh = Tensor([1,1,1])\n",
    "    size = 0.1\n",
    "    init_eff = 0.5\n",
    "    init_res = 1000\n",
    "    pos = 'above'\n",
    "    for z,d in zip(np.arange(lwh[2],0,-size), [1,1,0,0,0,0,0,0,1,1]):\n",
    "        if d:\n",
    "            layers.append(DetectorLayer(pos=pos, init_eff=init_eff, init_res=init_res,\n",
    "                                        lw=lwh[:2], z=z, size=size, eff_cost_func=eff_cost, res_cost_func=res_cost))\n",
    "        else:\n",
    "            pos = 'below'\n",
    "            layers.append(PassiveLayer(rad_length_func=arb_rad_length, lw=lwh[:2], z=z, size=size))\n",
    "\n",
    "    return nn.ModuleList(layers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = Volume(get_layers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VolumeWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    wrapper: Optional['VolumeWrapper'] = None\n",
    "        \n",
    "    def __init__(self): pass\n",
    "    def set_wrapper(self, wrapper:'VolumeWrapper') -> None: self.wrapper = wrapper\n",
    "    def set_plot_settings(self): pass\n",
    "\n",
    "    def on_train_begin(self) -> None:\n",
    "        if self.wrapper is None:\n",
    "            raise AttributeError(f\"The wrapper for {type(self).__name__} callback has not been set. Please call set_wrapper before on_train_begin.\")\n",
    "            \n",
    "    def on_train_end(self) -> None:   pass\n",
    "\n",
    "    def on_epoch_begin(self) -> None: pass\n",
    "    def on_epoch_end(self) -> None:   pass\n",
    "    \n",
    "    def on_volume_begin(self) -> None: pass\n",
    "    def on_volume_end(self) -> None:   pass\n",
    "\n",
    "    def on_mu_batch_begin(self) -> None: pass\n",
    "    def on_mu_batch_end(self) -> None:   pass\n",
    "\n",
    "    def on_scatter_end(self) -> None: pass\n",
    "\n",
    "    def on_backwards_begin(self) -> None: pass\n",
    "    def on_backwards_end(self) -> None:   pass\n",
    "    \n",
    "    def on_x0_pred_begin(self) -> None: pass\n",
    "    def on_x0_pred_end(self) -> None:   pass\n",
    "\n",
    "    def on_pred_begin(self) -> None: pass\n",
    "    def on_pred_end(self) -> None:   pass\n",
    "\n",
    "class CyclicCallback(Callback):\n",
    "    pass\n",
    "\n",
    "class MetricLogger(Callback):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lumin imports\n",
    "import inspect\n",
    "\n",
    "def is_partially(var:Any) -> bool:\n",
    "    r'''\n",
    "    Retuns true if var is partial, function, or class, else false.\n",
    "\n",
    "    Arguments:\n",
    "        var: variable to inspect\n",
    "\n",
    "    Return:\n",
    "        true if var is partial or partialler, else false\n",
    "    '''\n",
    "\n",
    "    return isinstance(var, (partial,types.FunctionType)) or inspect.isclass(var)\n",
    "\n",
    "class FitParams():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassiveGenerator():\n",
    "    pass\n",
    "\n",
    "    def generate(self) -> Callable[...,Tensor]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "class PassiveYielder():\n",
    "    def __init__(self, passives:Union[List[Callable[...,Tensor]],PassiveGenerator], n_passives:Optional[int]=None, shuffle:bool=True):\n",
    "        self.passives,self.n_passives,self.shuffle = passives,n_passives,shuffle\n",
    "        if isinstance(self.passives, PassiveGenerator):\n",
    "            self.generator = True\n",
    "            if self.n_passives is None:\n",
    "                raise ValueError('If a PassiveGenerator class is used, n_passives must be specified')\n",
    "        else:\n",
    "            self.generator = False\n",
    "            self.n_passives = len(self.passives)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.n_passives\n",
    "    \n",
    "    def __iter__(self) -> Callable[...,Tensor]:\n",
    "        if self.generator:\n",
    "            for _ in range(self.n_passives): yield self.passives.generate()\n",
    "        else:\n",
    "            if self.shuffle: shuffle(self.passives)\n",
    "            for p in self.passives: yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from fastcore.all import is_listy, Path\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "class VolumeWrapper():\n",
    "    def __init__(self, volume:Volume, res_opt:Callable[[Iterator[nn.Parameter]],torch.optim.Optimizer], eff_opt:Callable[[Iterator[nn.Parameter]],torch.optim.Optimizer],\n",
    "                 loss_func:Optional[nn.Module]=DetectorLoss, default_pred: Optional[float] = X0[\"beryllium\"]):\n",
    "        self.volume,self.loss_func,self.default_pred = volume,loss_func,default_pred\n",
    "        self._build_opt(res_opt, eff_opt)\n",
    "        self.parameters = self.volume.parameters\n",
    "        \n",
    "    def _build_opt(self, res_opt:Callable[[Iterator[nn.Parameter]],torch.optim.Optimizer], eff_opt:Callable[[Iterator[nn.Parameter]],torch.optim.Optimizer]) -> None:\n",
    "        self.res_opt = res_opt(((l.resolution for l in volume.get_detectors())))\n",
    "        self.eff_opt = eff_opt(((l.efficiency for l in volume.get_detectors())))\n",
    "        \n",
    "    def get_detectors(self) -> List[DetectorLayer]: return self.volume.get_detectors()\n",
    "    \n",
    "    def save(self, name:str) -> None:\n",
    "        torch.save({'volume':self.volume.state_dict(), 'res_opt':self.res_opt.state_dict(), 'eff_opt':self.eff_opt.state_dict()}, str(name))\n",
    "        \n",
    "    def load(self, name:str) -> None:\n",
    "        state = torch.load(name, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.volume.load_state_dict(state['volume'])\n",
    "        self.res_opt.load_state_dict(state['res_opt'])\n",
    "        self.eff_opt.load_state_dict(state['eff_opt'])\n",
    "    \n",
    "    @classmethod\n",
    "    def from_save(cls, name:str, volume:Volume, res_opt:Callable[[Iterator[nn.Parameter]],torch.optim.Optimizer], eff_opt:Callable[[Iterator[nn.Parameter]],torch.optim.Optimizer],\n",
    "                  loss_func:Optional[DetectorLoss], default_pred: Optional[float] = X0[\"beryllium\"]) -> VolumeWrapper:\n",
    "        vw = cls(volume=volume, res_opt=res_opt, eff_opt=eff_opt, loss_func=loss_func, default_pred=default_pred)\n",
    "        vw.load(name)\n",
    "        return vw\n",
    "    \n",
    "    def get_param_count(self, trainable:bool=True) -> int:\n",
    "        r'''\n",
    "        Return number of parameters in detector.\n",
    "\n",
    "        Arguments:\n",
    "            trainable: if true (default) only count trainable parameters\n",
    "\n",
    "        Returns:\n",
    "            Number of (trainable) parameters in detector\n",
    "        '''\n",
    "        \n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad or not trainable)\n",
    "    \n",
    "    def _scan_volume(self) -> None:\n",
    "        # Scan volume with muon batches\n",
    "        self.fit_params.wpreds, self.fit_params.weights = [], []\n",
    "        for _ in range(self.fit_params.n_mu_per_batch//self.fit_params.mu_bs):\n",
    "            self.fit_params.mu = MuonBatch(self.fit_params.mu_generator(self.fit_params.mu_bs), init_z=self.volume.h)\n",
    "            for c in self.fit_params.cbs: c.on_mu_batch_begin()\n",
    "            self.volume(self.fit_params.mu)\n",
    "            self.fit_params.sb = ScatterBatch(self.fit_params.mu, self.volume)\n",
    "            for c in self.fit_params.cbs: c.on_scatter_end()\n",
    "            inferer = X0Inferer(self.fit_params.sb, self.fit_params.default_pred)\n",
    "            pred, wgt = inferer.pred_x0(inc_default=False)\n",
    "            pred = torch.nan_to_num(pred)\n",
    "            self.fit_params.wpreds.append(pred*wgt)\n",
    "            self.fit_params.weights.append(wgt)\n",
    "            for c in self.fit_params.cbs: c.on_mu_batch_end()\n",
    "        \n",
    "        # Predict volme based on all muon batches\n",
    "        for c in self.fit_params.cbs: c.on_x0_pred_begin()\n",
    "        wgt = torch.stack(self.fit_params.weights, dim=0).sum(0)\n",
    "        pred = torch.stack(self.fit_params.wpreds, dim=0).sum(0)/wgt\n",
    "        pred, wgt = inferer.add_default_pred(pred, wgt)\n",
    "        self.fit_params.weight = wgt\n",
    "        self.fit_params.pred = pred\n",
    "        \n",
    "        for c in self.fit_params.cbs: c.on_x0_pred_end()\n",
    "        \n",
    "        # Compute loss for volume\n",
    "        if self.fit_params.state != 'test' and self.fit_params.loss_func is not None:\n",
    "            loss = self.fit_params.loss_func(pred_x0=self.fit_params.pred, pred_weight=self.fit_params.weight, volume=volume)\n",
    "            if self.fit_params.loss_val is None:\n",
    "                self.fit_params.loss_val = loss\n",
    "            else:\n",
    "                self.fit_params.loss_val = self.fit_params.loss_val+loss\n",
    "                \n",
    "    def _scan_volumes(self, passives:PassiveYielder) -> None:\n",
    "        self.fit_params.loss_val = None\n",
    "        for i, passive in enumerate(passives):\n",
    "            self.fit_params.volume_id = i\n",
    "            self.volume.load_rad_length(passive)\n",
    "            for c in self.fit_params.cbs: c.on_volume_begin()\n",
    "            self._scan_volume()\n",
    "            for c in self.fit_params.cbs: c.on_volume_end()\n",
    "        self.fit_params.mean_loss = self.fit_params.loss_val/len(passives)\n",
    "    \n",
    "    def fit(self, n_epochs:int, n_mu_per_batch:int, passive_bs:int, mu_bs:int, trn_passives:PassiveYielder, val_passives:Optional[PassiveYielder], mu_generator:Callable[[int],Tensor]=generate_batch,\n",
    "            cbs:Optional[Union[Callback,List[Callback]]]=None, cb_savepath:Path=Path('train_weights'),\n",
    "            visible_bar:bool=True) -> List[Callback]:\n",
    "        \n",
    "        if cbs is None: cbs = []\n",
    "        elif not is_listy(cbs): cbs = [cbs]\n",
    "        cyclic_cbs,loss_cbs,metric_log = [],[],None\n",
    "        for c in cbs:\n",
    "            if isinstance(c, CyclicCallback): cyclic_cbs.append(c)  # CBs that might prevent a wrapper from stopping training due to a hyper-param cycle\n",
    "            if hasattr(c, \"get_loss\"): loss_cbs.append(c)  # CBs that produce alternative losses that should be considered\n",
    "            if isinstance(c, MetricLogger): metric_log = c  # CB that logs losses and eval_metrics\n",
    "                \n",
    "        self.fit_params = FitParams(cbs=cbs, cyclic_cbs=cyclic_cbs, loss_cbs=loss_cbs, metric_log=metric_log, stop=False, n_epochs=n_epochs,\n",
    "                                    passive_bs=passive_bs, mu_bs=mu_bs, n_mu_per_batch=n_mu_per_batch, cb_savepath=Path(cb_savepath), trn_passives=trn_passives, val_passives=val_passives, mu_generator=mu_generator,\n",
    "                                    loss_func=self.loss_func, res_opt=self.res_opt, eff_opt=self.eff_opt, default_pred=self.default_pred)\n",
    "        self.fit_params.cb_savepath.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        def fit_epoch() -> None:\n",
    "            self.fit_params.epoch += 1\n",
    "            \n",
    "            # Training\n",
    "            self.volume.train()\n",
    "            self.fit_params.state = 'train'\n",
    "            for c in self.fit_params.cbs: c.on_epoch_begin()\n",
    "            self._scan_volumes(self.fit_params.trn_passives)  # Gain losses for all volumes\n",
    "            # Compute update step\n",
    "            self.fit_params.res_opt.zero_grad()\n",
    "            self.fit_params.eff_opt.zero_grad()\n",
    "            for c in self.fit_params.cbs: c.on_backwards_begin()\n",
    "            self.fit_params.loss_val.backward()\n",
    "            for c in self.fit_params.cbs: c.on_backwards_end()\n",
    "            self.fit_params.res_opt.step()     \n",
    "            self.fit_params.eff_opt.step()        \n",
    "            for c in self.fit_params.cbs: c.on_epoch_end()\n",
    "            \n",
    "            # Validation\n",
    "            if self.fit_params.val_passives is not None:\n",
    "                self.volume.eval()\n",
    "                for c in self.fit_params.cbs: c.on_epoch_begin()\n",
    "                self._scan_volumes(self.fit_params.val_passives)\n",
    "                for c in self.fit_params.cbs: c.on_epoch_end()\n",
    "                    \n",
    "        try:\n",
    "            for c in self.fit_params.cbs: c.set_wrapper(self)\n",
    "            for c in self.fit_params.cbs: c.on_train_begin()\n",
    "            for e in progress_bar(range(self.fit_params.n_epochs), display=visible_bar):\n",
    "                fit_epoch()\n",
    "                if self.fit_params.stop: break\n",
    "            for c in self.fit_params.cbs: c.on_train_end()\n",
    "        finally:\n",
    "            self.fit_params = None\n",
    "            torch.cuda.empty_cache()\n",
    "        return cbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoMoreNaN(Callback):\n",
    "    def on_backwards_end(self) -> None:\n",
    "        for l in self.wrapper.volume.get_detectors():\n",
    "            torch.nan_to_num_(l.resolution.grad, 0)\n",
    "            torch.nan_to_num_(l.efficiency.grad, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = Volume(get_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = VolumeWrapper(volume=volume, res_opt=partial(torch.optim.SGD, lr=2e1), eff_opt=partial(torch.optim.SGD, lr=2e-5), loss_func=DetectorLoss(0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_passives = PassiveYielder([arb_rad_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function arb_rad_length at 0x7fccb63edca0>\n"
     ]
    }
   ],
   "source": [
    "for p in trn_passives: print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='10' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [10/10 01:43<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X0 comp: 8563.88, cost comp: 6209\n",
      "X0 comp: 6342.17, cost comp: 6233\n",
      "X0 comp: 5576.31, cost comp: 6251\n",
      "X0 comp: 5460.25, cost comp: 6265\n",
      "X0 comp: 5391.18, cost comp: 6280\n",
      "X0 comp: 6661.94, cost comp: 6294\n",
      "X0 comp: 5537.45, cost comp: 6314\n",
      "X0 comp: 6883.91, cost comp: 6328\n",
      "X0 comp: 4103.39, cost comp: 6347\n",
      "X0 comp: 5390.42, cost comp: 6357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<__main__.NoMoreNaN at 0x7fcc8cb9a250>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.fit(10, n_mu_per_batch=1000, passive_bs=1, mu_bs=1000, trn_passives=trn_passives, val_passives=None, cbs=[NoMoreNaN()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tomopt]",
   "language": "python",
   "name": "conda-env-tomopt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
