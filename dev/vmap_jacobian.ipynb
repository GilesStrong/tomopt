{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.muon import generate_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.muon import MuonBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.core import X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arb_rad_length(*,z:float, lw:Tensor, size:float) -> float:\n",
    "    rad_length = torch.ones(list((lw/size).long()))*1e5\n",
    "    if z >= 0.5 and z <= 0.5: rad_length[...] = X0['lead']#X0['beryllium']\n",
    "#     if z == 0.6 : rad_length[...] = X0['beryllium']\n",
    "        \n",
    "    return rad_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.volume import PassiveLayer, DetectorLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eff_cost(x:Tensor) -> Tensor:\n",
    "    return torch.expm1(3*F.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_cost(x:Tensor) -> Tensor:\n",
    "    return F.relu(x/100)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers():\n",
    "    layers = []\n",
    "    lwh = Tensor([1,1,1])\n",
    "    size = 0.1\n",
    "    init_eff = 0.5\n",
    "    init_res = 100000\n",
    "    pos = 'above'\n",
    "    for z,d in zip(np.arange(lwh[2],0,-size), [1,1,0,0,0,0,0,0,1,1]):\n",
    "        if d:\n",
    "            layers.append(DetectorLayer(pos=pos, init_eff=init_eff, init_res=init_res,\n",
    "                                        lw=lwh[:2], z=z, size=size, eff_cost_func=eff_cost, res_cost_func=res_cost))\n",
    "        else:\n",
    "            pos = 'below'\n",
    "            layers.append(PassiveLayer(rad_length_func=arb_rad_length, lw=lwh[:2], z=z, size=size))\n",
    "\n",
    "    return nn.ModuleList(layers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.volume import Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = MuonBatch(generate_batch(1000), init_z=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = Volume(get_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 1.8.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /Users/giles/anaconda3/envs/tomopt/lib/python3.8/site-packages\n",
      "Requires: typing-extensions, numpy\n",
      "Required-by: torchvision, torchaudio, tomopt\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._vmap_internals import _vmap as vmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8373, -1.5334])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot                            # [D], [D] -> []\n",
    "batched_dot = vmap(torch.dot)  # [N, D], [N, D] -> [N]\n",
    "x, y = torch.randn(2, 5), torch.randn(2, 5)\n",
    "batched_dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 3.7810], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, feature_size = 3, 5\n",
    "weights = torch.randn(feature_size, requires_grad=True)\n",
    "\n",
    "def model(feature_vec):\n",
    "    # Very simple linear model with activation\n",
    "    return feature_vec.dot(weights).relu()\n",
    "\n",
    "examples = torch.randn(batch_size, feature_size)\n",
    "result = vmap(model)(examples)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3702,  0.0000,  0.0000,  0.0000, -0.0000],\n",
       "        [ 0.0000,  0.2037,  0.0000,  0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000,  2.1707,  0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0080, -0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.6036]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup\n",
    "N = 5\n",
    "f = lambda x: x ** 2\n",
    "x = torch.randn(N, requires_grad=True)\n",
    "y = f(x)\n",
    "I_N = torch.eye(N)\n",
    "\n",
    "# Sequential approach\n",
    "jacobian_rows = [torch.autograd.grad(y, x, v, retain_graph=True)[0]\n",
    "                 for v in I_N.unbind()]\n",
    "jacobian = torch.stack(jacobian_rows)\n",
    "jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dx = lambda x: 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3702,  0.2037,  2.1707,  0.0080, -1.6036], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dx(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3702,  0.0000,  0.0000,  0.0000, -0.0000],\n",
       "        [ 0.0000,  0.2037,  0.0000,  0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000,  2.1707,  0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0080, -0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.6036]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorized gradient computation\n",
    "def get_vjp(v):\n",
    "    return torch.autograd.grad(y, x, v, retain_graph=True)[0]\n",
    "jacobian = vmap(get_vjp)(I_N)\n",
    "jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 0., 0., 0., 0.]),\n",
       " tensor([0., 1., 0., 0., 0.]),\n",
       " tensor([0., 0., 1., 0., 0.]),\n",
       " tensor([0., 0., 0., 1., 0.]),\n",
       " tensor([0., 0., 0., 0., 1.]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_N.unbind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1851,  0.1018,  1.0853,  0.0040, -0.8018], requires_grad=True),\n",
       " tensor([3.4271e-02, 1.0372e-02, 1.1780e+00, 1.6123e-05, 6.4291e-01],\n",
       "        grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch-wise grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 µs ± 1.05 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "f = lambda x: x ** 2\n",
    "x = torch.randn(N, requires_grad=True)\n",
    "y = f(x)\n",
    "I_N = torch.eye(N)\n",
    "\n",
    "%timeit torch.stack([torch.autograd.grad(y,x,v, retain_graph=True)[0] for v in I_N.unbind()]).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.4 µs ± 3.44 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def get_vjp(v): return torch.autograd.grad(y,x,v, retain_graph=True)[0].sum()\n",
    "vmap(get_vjp)(I_N)\n",
    "\n",
    "%timeit vmap(get_vjp)(I_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchwise_grad(y: Tensor, x: Tensor, create_graph: bool = False, allow_unused: bool = True) -> Tensor:\n",
    "    def get_vjp(v): return torch.autograd.grad(y, x, v, retain_graph=True, create_graph=create_graph, allow_unused=allow_unused)[0].sum()\n",
    "    return vmap(get_vjp)(torch.eye(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6276,  2.7733, -3.3634, -0.5657,  0.0976])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchwise_grad(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch-wise jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5066,  0.0000, -0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  1.5566, -0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.5869, -0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000, -1.1729,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000, -0.0000,  5.8094]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "f = lambda x: x ** 2\n",
    "x = torch.randn(N, requires_grad=True)\n",
    "y = f(x)\n",
    "I_N = torch.eye(N)\n",
    "\n",
    "yb = torch.stack([y for _ in range(5)])\n",
    "\n",
    "torch.stack([torch.autograd.grad(y,x,v, retain_graph=True)[0] for v in I_N.unbind()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_y = yb.reshape(-1)\n",
    "I_N = torch.eye(len(flat_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29 ms ± 14.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "jac = []\n",
    "for grad_y in I_N.unbind():\n",
    "    (grad_x,) = torch.autograd.grad(flat_y, x, grad_y, retain_graph=True)\n",
    "    jac.append(grad_x.reshape(x.shape))\n",
    "torch.stack(jac).reshape(yb.shape + x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.5066,  0.0000, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  1.5566, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.5869, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000, -1.1729,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000, -0.0000,  5.8094]],\n",
       "\n",
       "        [[ 4.5066,  0.0000, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  1.5566, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.5869, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000, -1.1729,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000, -0.0000,  5.8094]],\n",
       "\n",
       "        [[ 4.5066,  0.0000, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  1.5566, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.5869, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000, -1.1729,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000, -0.0000,  5.8094]],\n",
       "\n",
       "        [[ 4.5066,  0.0000, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  1.5566, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.5869, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000, -1.1729,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000, -0.0000,  5.8094]],\n",
       "\n",
       "        [[ 4.5066,  0.0000, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  1.5566, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.5869, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000, -1.1729,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000, -0.0000,  5.8094]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vjp(v): return torch.autograd.grad(flat_y, x, v, retain_graph=True)[0].reshape(x.shape)\n",
    "vmap(get_vjp)(I_N).reshape(yb.shape + x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchwise_jacobian(y: Tensor, x: Tensor, create_graph: bool = False, allow_unused: bool = True) -> Tensor:\n",
    "    flat_y = y.reshape(-1)\n",
    "\n",
    "    def get_vjp(v): return torch.autograd.grad(flat_y, x, v, retain_graph=True, create_graph=create_graph, allow_unused=allow_unused)[0].reshape(x.shape)\n",
    "    \n",
    "    return vmap(get_vjp)(torch.eye(len(flat_y))).reshape(y.shape + x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5066,  0.0000, -0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  1.5566, -0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.5869, -0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000, -1.1729,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000, -0.0000,  5.8094]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchwise_jacobian(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 µs ± 1.89 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit batchwise_jacobian(yb, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomopt.inference import ScatterBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_jacobian(y: Tensor, x: Tensor, create_graph: bool = False, allow_unused: bool = True) -> Tensor:\n",
    "    jac = []\n",
    "    flat_y = y.reshape(-1)\n",
    "    for grad_y in torch.eye(len(flat_y)).unbind():\n",
    "        (grad_x,) = torch.autograd.grad(flat_y, x, grad_y, retain_graph=True, allow_unused=allow_unused)\n",
    "        jac.append(grad_x.reshape(x.shape))\n",
    "    return torch.stack(jac).reshape(y.shape + x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForScatterBatch(ScatterBatch):\n",
    "    def compute_scatters(self) -> None:\n",
    "        r\"\"\"\n",
    "        Currently only handles 2 detectors above and below passive volume\n",
    "        Scatter locations adapted from:\n",
    "        @MISC {3334866,\n",
    "            TITLE = {Closest points between two lines},\n",
    "            AUTHOR = {Brian (https://math.stackexchange.com/users/72614/brian)},\n",
    "            HOWPUBLISHED = {Mathematics Stack Exchange},\n",
    "            NOTE = {URL:https://math.stackexchange.com/q/3334866 (version: 2019-08-26)},\n",
    "            EPRINT = {https://math.stackexchange.com/q/3334866},\n",
    "            URL = {https://math.stackexchange.com/q/3334866}\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        # self.hits in layers\n",
    "        xa0 = torch.cat([self.hits[\"above\"][\"xy\"][:, 0], self.hits[\"above\"][\"z\"][:, 0]], dim=-1)  # reco x, reco y, gen z\n",
    "        xa1 = torch.cat([self.hits[\"above\"][\"xy\"][:, 1], self.hits[\"above\"][\"z\"][:, 1]], dim=-1)\n",
    "        xb0 = torch.cat([self.hits[\"below\"][\"xy\"][:, 1], self.hits[\"below\"][\"z\"][:, 1]], dim=-1)\n",
    "        xb1 = torch.cat([self.hits[\"below\"][\"xy\"][:, 0], self.hits[\"below\"][\"z\"][:, 0]], dim=-1)\n",
    "\n",
    "        dets = self.volume.get_detectors()\n",
    "        res = []\n",
    "        for p, l, i in zip((\"above\", \"above\", \"below\", \"below\"), dets, (0, 1, 0, 1)):\n",
    "            x = l.abs2idx(self.hits[p][\"xy\"][:, i])\n",
    "            res.append(l.resolution[x[:, 0], x[:, 1]])\n",
    "        res2 = torch.stack(res, dim=1)[:, :, None] ** 2\n",
    "\n",
    "        # Extrapolate muon-path vectors from self.hits\n",
    "        v1 = xa1 - xa0\n",
    "        v2 = xb1 - xb0\n",
    "\n",
    "        # scatter locations\n",
    "        v3 = torch.cross(v1, v2, dim=1)  # connecting vector perpendicular to both lines\n",
    "        rhs = xb0 - xa0\n",
    "        lhs = torch.stack([v1, -v2, v3], dim=1).transpose(2, 1)\n",
    "        coefs = torch.linalg.solve(lhs, rhs)  # solve p1+t1*v1 + t3*v3 = p2+t2*v2 => p2-p1 = t1*v1 - t2*v2 + t3*v3\n",
    "\n",
    "        q1 = xa0 + (coefs[:, 0:1] * v1)  # closest point on v1\n",
    "        self._loc = q1 + (coefs[:, 2:3] * v3 / 2)  # Move halfway along v3 from q1\n",
    "\n",
    "        # Theta deviations\n",
    "        self._theta_in = torch.arctan(v1[:, :2] / v1[:, 2:3])\n",
    "        self._theta_out = torch.arctan(v2[:, :2] / v2[:, 2:3])\n",
    "        self._dtheta = torch.abs(self._theta_in - self._theta_out)\n",
    "\n",
    "        # xy deviations\n",
    "        self._dxy = coefs[:, 2:3] * v3[:, :2]\n",
    "\n",
    "        # loc uncertainty\n",
    "        dloc_dres = torch.stack([for_jacobian(self._loc, l.resolution).sum((-1, -2)) for l in dets], dim=1)\n",
    "        self._loc_unc = torch.sqrt((dloc_dres.pow(2) * res2).sum(1))\n",
    "\n",
    "        # dtheta uncertainty\n",
    "        ddtheta_dres = torch.stack([for_jacobian(self._dtheta, l.resolution).sum((-1, -2)) for l in dets], dim=1)\n",
    "        self._dtheta_unc = torch.sqrt((ddtheta_dres.pow(2) * res2).sum(1))\n",
    "\n",
    "        # dxy uncertainty\n",
    "        ddxy_dres = torch.stack([for_jacobian(self._dxy, l.resolution).sum((-1, -2)) for l in dets], dim=1)\n",
    "        self._dxy_unc = torch.sqrt((ddxy_dres.pow(2) * res2).sum(1))\n",
    "\n",
    "        # theta_in uncertainty\n",
    "        dtheta_in_dres = torch.stack([for_jacobian(self._theta_in, l.resolution).sum((-1, -2)) for l in dets[:2]], dim=1)\n",
    "        self._theta_in_unc = torch.sqrt((dtheta_in_dres.pow(2) * res2[:, :2]).sum(1))\n",
    "\n",
    "        # theta_out uncertainty\n",
    "        dtheta_out_dres = torch.stack([for_jacobian(self._theta_out, l.resolution).sum((-1, -2)) for l in dets[2:]], dim=1)\n",
    "        self._theta_out_unc = torch.sqrt((dtheta_out_dres.pow(2) * res2[:, 2:]).sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.9 s, sys: 768 ms, total: 20.6 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%time scatters = ForScatterBatch(batch, volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchwise grad scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BWScatterBatch(ScatterBatch):\n",
    "    def compute_scatters(self) -> None:\n",
    "        r\"\"\"\n",
    "        Currently only handles 2 detectors above and below passive volume\n",
    "\n",
    "        Scatter locations adapted from:\n",
    "        @MISC {3334866,\n",
    "            TITLE = {Closest points between two lines},\n",
    "            AUTHOR = {Brian (https://math.stackexchange.com/users/72614/brian)},\n",
    "            HOWPUBLISHED = {Mathematics Stack Exchange},\n",
    "            NOTE = {URL:https://math.stackexchange.com/q/3334866 (version: 2019-08-26)},\n",
    "            EPRINT = {https://math.stackexchange.com/q/3334866},\n",
    "            URL = {https://math.stackexchange.com/q/3334866}\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        # self.hits in layers\n",
    "        xa0 = torch.cat([self.hits[\"above\"][\"xy\"][:, 0], self.hits[\"above\"][\"z\"][:, 0]], dim=-1)  # reco x, reco y, gen z\n",
    "        xa1 = torch.cat([self.hits[\"above\"][\"xy\"][:, 1], self.hits[\"above\"][\"z\"][:, 1]], dim=-1)\n",
    "        xb0 = torch.cat([self.hits[\"below\"][\"xy\"][:, 1], self.hits[\"below\"][\"z\"][:, 1]], dim=-1)\n",
    "        xb1 = torch.cat([self.hits[\"below\"][\"xy\"][:, 0], self.hits[\"below\"][\"z\"][:, 0]], dim=-1)\n",
    "\n",
    "        dets = self.volume.get_detectors()\n",
    "        res = []\n",
    "        for p, l, i in zip((\"above\", \"above\", \"below\", \"below\"), dets, (0, 1, 0, 1)):\n",
    "            x = l.abs2idx(self.hits[p][\"xy\"][:, i])\n",
    "            res.append(l.resolution[x[:, 0], x[:, 1]])\n",
    "        res2 = torch.stack(res, dim=1)[:, :, None] ** 2\n",
    "\n",
    "        # Extrapolate muon-path vectors from self.hits\n",
    "        v1 = xa1 - xa0\n",
    "        v2 = xb1 - xb0\n",
    "\n",
    "        # scatter locations\n",
    "        v3 = torch.cross(v1, v2, dim=1)  # connecting vector perpendicular to both lines\n",
    "        rhs = xb0 - xa0\n",
    "        lhs = torch.stack([v1, -v2, v3], dim=1).transpose(2, 1)\n",
    "        coefs = torch.linalg.solve(lhs, rhs)  # solve p1+t1*v1 + t3*v3 = p2+t2*v2 => p2-p1 = t1*v1 - t2*v2 + t3*v3\n",
    "\n",
    "        q1 = xa0 + (coefs[:, 0:1] * v1)  # closest point on v1\n",
    "        self._loc = q1 + (coefs[:, 2:3] * v3 / 2)  # Move halfway along v3 from q1\n",
    "        \n",
    "        \n",
    "        # Theta deviations\n",
    "        self._theta_in = torch.arctan(v1[:, :2] / v1[:, 2:3])\n",
    "        self._theta_out = torch.arctan(v2[:, :2] / v2[:, 2:3])\n",
    "        self._dtheta = torch.abs(self._theta_in - self._theta_out)\n",
    "\n",
    "        # xy deviations\n",
    "        self._dxy = coefs[:, 2:3] * v3[:, :2]\n",
    "        \n",
    "        # loc uncertainty\n",
    "        dloc_dres = torch.stack([batchwise_jacobian(self._loc, l.resolution).sum((-1, -2)) for l in dets], dim=1)\n",
    "        self._loc_unc = torch.sqrt((dloc_dres.pow(2) * res2).sum(1))\n",
    "\n",
    "        # dtheta uncertainty\n",
    "        ddtheta_dres = torch.stack([batchwise_jacobian(self._dtheta, l.resolution).sum((-1, -2)) for l in dets], dim=1)\n",
    "        self._dtheta_unc = torch.sqrt((ddtheta_dres.pow(2) * res2).sum(1))\n",
    "\n",
    "        # dxy uncertainty\n",
    "        ddxy_dres = torch.stack([batchwise_jacobian(self._dxy, l.resolution).sum((-1, -2)) for l in dets], dim=1)\n",
    "        self._dxy_unc = torch.sqrt((ddxy_dres.pow(2) * res2).sum(1))\n",
    "\n",
    "        # theta_in uncertainty\n",
    "        dtheta_in_dres = torch.stack([batchwise_jacobian(self._theta_in, l.resolution).sum((-1, -2)) for l in dets[:2]], dim=1)\n",
    "        self._theta_in_unc = torch.sqrt((dtheta_in_dres.pow(2) * res2[:, :2]).sum(1))\n",
    "\n",
    "        # theta_out uncertainty\n",
    "        dtheta_out_dres = torch.stack([batchwise_jacobian(self._theta_out, l.resolution).sum((-1, -2)) for l in dets[2:]], dim=1)\n",
    "        self._theta_out_unc = torch.sqrt((dtheta_out_dres.pow(2) * res2[:, 2:]).sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 s, sys: 1.5 s, total: 13.8 s\n",
      "Wall time: 9.45 s\n"
     ]
    }
   ],
   "source": [
    "%time scatters = BWScatterBatch(batch, volume)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tomopt]",
   "language": "python",
   "name": "conda-env-tomopt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
